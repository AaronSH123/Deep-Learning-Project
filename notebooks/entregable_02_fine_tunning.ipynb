{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be16e79f",
   "metadata": {},
   "source": [
    "# Proyecto Final - Deep Learning (1INF52)\n",
    "## Entregable 2: Fine-tuning de T5 para generaci贸n de res煤menes\n",
    "\n",
    "**Integrantes:** *\n",
    "\n",
    "Aar贸n Ulises Santill谩n Huam谩n - 20200445\n",
    "\n",
    "> Este cuaderno implementa el pipeline de fine-tuning del modelo T5 sobre el dataset CNN/DailyMail\n",
    "usando la librer铆a  Hugging Face. Incluye carga de datos, preprocesamiento, entrenamiento,\n",
    "evaluaci贸n con m茅tricas ROUGE y guardado del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384feb41",
   "metadata": {},
   "source": [
    "## 1. Instalaci贸n de dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54c166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si est谩s en Google Colab, descomenta y ejecuta esta celda una sola vez.\n",
    "!pip install -q transformers datasets accelerate rouge-score evaluate sentencepiece\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c940575",
   "metadata": {},
   "source": [
    "## 2. Importaciones y configuraci贸n inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2437db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5TokenizerFast,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "# Verificar dispositivo disponible\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Dispositivo: {device}\")\n",
    "\n",
    "# Fijar semilla para reproducibilidad\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fda6c57",
   "metadata": {},
   "source": [
    "## 3. Carga del dataset CNN/DailyMail\n",
    "\n",
    "Usaremos la versi贸n `3.0.0` del dataset `cnn_dailymail` disponible en Hugging Face Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68190a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset CNN/DailyMail\n",
    "dataset = load_dataset('cnn_dailymail', '3.0.0')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6c15cb",
   "metadata": {},
   "source": [
    "## 4. Carga del tokenizer y del modelo T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ce0829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puedes cambiar 't5-small' por 't5-base' si tienes suficientes recursos\n",
    "model_name = 't5-small'\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "model.to(device);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096210aa",
   "metadata": {},
   "source": [
    "## 5. Preprocesamiento de datos\n",
    "Convertimos cada ejemplo en pares `(input_ids, labels)` para el modelo T5.\n",
    "\n",
    "- Entrada: prefijo `summarize: ` + art铆culo\n",
    "- Salida: `highlights` (resumen humano)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c4bc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 512   # longitud m谩xima del art铆culo\n",
    "max_target_length = 128  # longitud m谩xima del resumen\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    # Prefijo de tarea para T5\n",
    "    inputs = [\"summarize: \" + doc for doc in batch['article']]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "    )\n",
    "\n",
    "    # Tokenizaci贸n de los res煤menes (labels)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch['highlights'],\n",
    "            max_length=max_target_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "        )\n",
    "\n",
    "    # Reemplazar tokens de padding con -100 para que no contribuyan a la p茅rdida\n",
    "    labels_ids = labels['input_ids']\n",
    "    labels_ids = [\n",
    "        [(lid if lid != tokenizer.pad_token_id else -100) for lid in label]\n",
    "        for label in labels_ids\n",
    "    ]\n",
    "    model_inputs['labels'] = labels_ids\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86641d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para reducir tiempos de prueba, puedes usar un subconjunto del dataset\n",
    "small_train = dataset['train'].shuffle(seed=seed).select(range(20000))  # ajustar seg煤n recursos\n",
    "small_val = dataset['validation'].shuffle(seed=seed).select(range(2000))\n",
    "\n",
    "tokenized_train = small_train.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names,\n",
    ")\n",
    "tokenized_val = small_val.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['validation'].column_names,\n",
    ")\n",
    "tokenized_train, tokenized_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2cdcd9",
   "metadata": {},
   "source": [
    "## 6. Definici贸n de la m茅trica ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae75fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Reemplazar -100 por el token de padding\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,\n",
    "    )\n",
    "    # Promediar y multiplicar por 100 para dejar en porcentaje\n",
    "    result = {k: round(v * 100, 2) for k, v in result.items()}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0e1265",
   "metadata": {},
   "source": [
    "## 7. Configuraci贸n de entrenamiento (TrainingArguments + Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b20cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './t5_cnn_summarization'\n",
    "\n",
    "batch_size = 4  # ajustar seg煤n memoria de GPU/CPU\n",
    "num_epochs = 3\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=num_epochs,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ef8cc7",
   "metadata": {},
   "source": [
    "## 8. Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25adad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = trainer.train()\n",
    "trainer.save_model(output_dir)\n",
    "train_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6436732f",
   "metadata": {},
   "source": [
    "## 9. Evaluaci贸n en el conjunto de validaci贸n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f72605",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate()\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c0b2dd",
   "metadata": {},
   "source": [
    "## 10. Ejemplos de res煤menes generados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b38416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(article, max_new_tokens=128):\n",
    "    # Construimos el prompt para T5\n",
    "    input_text = \"summarize: \" + article\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        max_length=max_input_length,\n",
    "    ).to(device)\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Tomamos un ejemplo del conjunto de validaci贸n original (no tokenizado)\n",
    "example = dataset['validation'][0]\n",
    "print('=== ARTCULO ORIGINAL ===')\n",
    "print(example['article'][:1000], '...')\n",
    "\n",
    "print('\\n=== RESUMEN HUMANO (highlights) ===')\n",
    "print(example['highlights'])\n",
    "\n",
    "print('\\n=== RESUMEN GENERADO POR EL MODELO ===')\n",
    "generated = generate_summary(example['article'])\n",
    "print(generated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbf78fb",
   "metadata": {},
   "source": [
    "## 11. (Opcional) Publicar el modelo en Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43042dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar esta celda solo si quieres subir tu modelo al Hub.\n",
    "# 1) Primero: !huggingface-cli login (en la terminal/colab)\n",
    "# 2) Luego define tu nombre de repo y ejecuta.\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "print('Si est谩s en Colab/Jupyter, puedes autenticarte con: notebook_login()')\n",
    "print('Luego usa: trainer.push_to_hub(\"nombre-usuario/t5-cnn-summarization\")')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
